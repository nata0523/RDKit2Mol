{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "# === Config === #\n",
    "class MolGPTConfig(PretrainedConfig):\n",
    "    model_type = \"molgpt\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size=100,\n",
    "                 embedding_dim=256,\n",
    "                 padding_idx=0,\n",
    "                 n_heads=8,\n",
    "                 feedforward_dim=512,\n",
    "                 n_layers=6,\n",
    "                 desc_size=166,\n",
    "                 max_position_embeddings=512,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.desc_size = desc_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "\n",
    "\n",
    "# === Descriptor Encoder === #\n",
    "class DescriptorEncoder(nn.Module):\n",
    "    def __init__(self, desc_size, emb_dim, dropout=0.1, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.desc_size = desc_size\n",
    "        self.device = device\n",
    "        self.bit_emb = nn.Embedding(desc_size, emb_dim)\n",
    "        self.scale_norm = nn.LayerNorm(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, desc):  # desc: [B, D], float\n",
    "        B, D = desc.shape\n",
    "        assert D == self.desc_size, \"Input descriptor dim must match initialized size\"\n",
    "        bit_idx = torch.arange(D, device=desc.device).unsqueeze(0).expand(B, D)  # [B, D]\n",
    "        bit = self.bit_emb(bit_idx)                  # [B, D, emb]\n",
    "        val = self.scale_norm(desc.unsqueeze(-1))    # [B, D, 1]\n",
    "        emb = self.dropout(bit * val)                # [B, D, emb]\n",
    "        return emb\n",
    "\n",
    "\n",
    "# === Embedding + Positional Encoding === #\n",
    "class MolGPTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=config.padding_idx)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "\n",
    "# === Transformer Stack === #\n",
    "class MolGPTTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=config.embedding_dim,\n",
    "            nhead=config.n_heads,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=config.n_layers)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        return self.encoder(x, mask=attn_mask)\n",
    "\n",
    "\n",
    "# === Causal Mask === #\n",
    "def generate_causal_mask(seq_len, device):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
    "    return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "\n",
    "# === Main Model === #\n",
    "class MolGPTModel(PreTrainedModel):\n",
    "    config_class = MolGPTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = MolGPTEmbeddings(config)\n",
    "        self.descriptor_encoder = DescriptorEncoder(config.desc_size, config.embedding_dim)\n",
    "        self.transformer = MolGPTTransformer(config)\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, descriptors, labels=None):\n",
    "        token_emb = self.embeddings(input_ids)  # [B, T, D]\n",
    "        desc_emb = self.descriptor_encoder(descriptors)  # [B, D1, D]\n",
    "        x = torch.cat([desc_emb, token_emb], dim=1)  # [B, D1+T, D]\n",
    "\n",
    "        attn_mask = generate_causal_mask(x.size(1), x.device)\n",
    "        x = self.transformer(x, attn_mask=attn_mask)\n",
    "\n",
    "        logits = self.lm_head(x)  # [B, D1+T, V]\n",
    "        token_logits = logits[:, desc_emb.size(1):, :]  # token part only\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(token_logits.view(-1, token_logits.size(-1)), labels.view(-1))\n",
    "            return {\"loss\": loss, \"logits\": token_logits}\n",
    "\n",
    "        return {\"logits\": token_logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "\n",
    "# === Config === #\n",
    "class MolGPTConfig(PretrainedConfig):\n",
    "    model_type = \"molgpt\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size=100,\n",
    "                 embedding_dim=256,\n",
    "                 padding_idx=0,\n",
    "                 n_heads=8,\n",
    "                 feedforward_dim=512,\n",
    "                 n_layers=6,\n",
    "                 desc_size=166,\n",
    "                 max_position_embeddings=512,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.desc_size = desc_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "\n",
    "\n",
    "# === Descriptor Encoder === #\n",
    "class DescriptorEncoder(nn.Module):\n",
    "    def __init__(self, desc_size, emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.desc_size = desc_size\n",
    "        self.bit_emb = nn.Embedding(desc_size, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, desc):  # desc: [B, D], float\n",
    "        B, D = desc.shape\n",
    "        assert D == self.desc_size, \"Input descriptor dim must match initialized size\"\n",
    "        bit_idx = torch.arange(D, device=desc.device).unsqueeze(0).expand(B, D)  # [B, D]\n",
    "        bit = self.bit_emb(bit_idx)                   # [B, D, emb]\n",
    "        desc_norm = (desc - desc.mean(dim=-1, keepdim=True)) / (desc.std(dim=-1, keepdim=True) + 1e-6)\n",
    "        val = desc_norm.unsqueeze(-1)                 # [B, D, 1]\n",
    "        emb = self.dropout(bit * val)                 # [B, D, emb]\n",
    "        return emb\n",
    "\n",
    "\n",
    "# === Embedding + Positional Encoding === #\n",
    "class MolGPTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=config.padding_idx)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "\n",
    "# === Transformer Stack === #\n",
    "class MolGPTTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=config.embedding_dim,\n",
    "            nhead=config.n_heads,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=config.n_layers)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        return self.encoder(x, attn_mask=attn_mask)\n",
    "\n",
    "\n",
    "# === Causal Mask === #\n",
    "def generate_causal_mask(seq_len, device):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "    return mask  # bool型にすることで TransformerEncoder で使いやすくする\n",
    "\n",
    "\n",
    "# === Main Model === #\n",
    "class MolGPTModel(PreTrainedModel):\n",
    "    config_class = MolGPTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = MolGPTEmbeddings(config)\n",
    "        self.descriptor_encoder = DescriptorEncoder(config.desc_size, config.embedding_dim, dropout=config.hidden_dropout_prob)\n",
    "        self.transformer = MolGPTTransformer(config)\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, descriptors: torch.Tensor, labels: torch.Tensor = None) -> CausalLMOutput:\n",
    "        token_emb = self.embeddings(input_ids)               # [B, T, D]\n",
    "        desc_emb = self.descriptor_encoder(descriptors)      # [B, D1, D]\n",
    "        x = torch.cat([desc_emb, token_emb], dim=1)          # [B, D1+T, D]\n",
    "\n",
    "        attn_mask = generate_causal_mask(x.size(1), x.device)\n",
    "        x = self.transformer(x, attn_mask=attn_mask)\n",
    "\n",
    "        logits = self.lm_head(x)                             # [B, D1+T, V]\n",
    "        token_logits = logits[:, desc_emb.size(1):, :]       # token part only\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(token_logits.view(-1, token_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return CausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=token_logits\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "\n",
    "# === Config === #\n",
    "class MolGPTConfig(PretrainedConfig):\n",
    "    model_type = \"molgpt\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size=100,\n",
    "                 embedding_dim=256,\n",
    "                 padding_idx=0,\n",
    "                 n_heads=8,\n",
    "                 feedforward_dim=512,\n",
    "                 n_layers=6,\n",
    "                 desc_size=166,\n",
    "                 max_position_embeddings=512,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.desc_size = desc_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "\n",
    "\n",
    "# === Descriptor Encoder === #\n",
    "class DescriptorEncoder(nn.Module):\n",
    "    def __init__(self, desc_size, emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.desc_size = desc_size\n",
    "        self.bit_emb = nn.Embedding(desc_size, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, desc):  # desc: [B, D], float\n",
    "        B, D = desc.shape\n",
    "        assert D == self.desc_size, \"Input descriptor dim must match initialized size\"\n",
    "        bit_idx = torch.arange(D, device=desc.device).unsqueeze(0).expand(B, D)\n",
    "        bit = self.bit_emb(bit_idx)  # [B, D, emb]\n",
    "        desc_norm = (desc - desc.mean(dim=-1, keepdim=True)) / (desc.std(dim=-1, keepdim=True) + 1e-6)\n",
    "        val = desc_norm.unsqueeze(-1)  # [B, D, 1]\n",
    "        emb = self.dropout(bit * val)  # [B, D, emb]\n",
    "        return emb\n",
    "\n",
    "\n",
    "# === Embedding + Positional Encoding === #\n",
    "class MolGPTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=config.padding_idx)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "\n",
    "# === Transformer Stack === #\n",
    "class MolGPTTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=config.embedding_dim,\n",
    "            nhead=config.n_heads,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=config.n_layers)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        return self.encoder(x, mask=attn_mask)\n",
    "\n",
    "\n",
    "# === Causal Mask === #\n",
    "def generate_causal_mask(seq_len, device):\n",
    "    mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# === Main Model === #\n",
    "class MolGPTModel(PreTrainedModel):\n",
    "    config_class = MolGPTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = MolGPTEmbeddings(config)\n",
    "        self.descriptor_encoder = DescriptorEncoder(config.desc_size, config.embedding_dim, dropout=config.hidden_dropout_prob)\n",
    "        self.transformer = MolGPTTransformer(config)\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "        self.post_init()  # ← init_weights() ではなくこちらを使用\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, descriptors: torch.Tensor, labels: torch.Tensor = None) -> CausalLMOutput:\n",
    "        token_emb = self.embeddings(input_ids)               # [B, T, D]\n",
    "        desc_emb = self.descriptor_encoder(descriptors)      # [B, D1, D]\n",
    "        x = torch.cat([desc_emb, token_emb], dim=1)          # [B, D1+T, D]\n",
    "\n",
    "        attn_mask = generate_causal_mask(x.size(1), x.device)  # [L, L]\n",
    "        x = self.transformer(x, attn_mask=attn_mask)\n",
    "\n",
    "        logits = self.lm_head(x)                             # [B, D1+T, V]\n",
    "        token_logits = logits[:, desc_emb.size(1):, :]       # token part only\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.padding_idx)\n",
    "            loss = loss_fct(token_logits.view(-1, token_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return CausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=token_logits\n",
    "        )\n",
    "\n",
    "    def generate(self, input_ids, descriptors, max_length=100):\n",
    "        self.eval()\n",
    "        generated = input_ids\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.forward(generated, descriptors)\n",
    "                next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "                if (next_token == self.config.padding_idx).all():\n",
    "                    break\n",
    "        return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import re\n",
    "\n",
    "class SMILESTokenizerHF(PreTrainedTokenizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(\n",
    "            pad_token='<PAD>',\n",
    "            bos_token='<BOS>',\n",
    "            eos_token='<EOS>',\n",
    "            unk_token='<UNK>',\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # 固定トークンセット\n",
    "        self.tokens = ['#', '%10', '%11', '%12', '(', ')', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S',\n",
    "            '[B-]', '[BH-]', '[BH2-]', '[BH3-]', '[B]', '[C+]', '[C-]', '[CH+]', '[CH-]', '[CH2+]', '[CH2]', '[CH]', '[F+]', '[H]', '[I+]', '[IH2]', '[IH]', '[N+]', '[N-]',\n",
    "            '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[N]', '[O+]', '[O-]', '[OH+]', '[O]', '[P+]', '[PH+]', '[PH2+]', '[PH]', '[S+]', '[S-]', '[SH+]', '[SH]', '[Se+]', \n",
    "            '[SeH+]', '[SeH]', '[Se]', '[Si-]', '[SiH-]', '[SiH2]', '[SiH]', '[Si]', '[b-]', '[bH-]', '[c+]', '[c-]', '[cH+]', '[cH-]', '[n+]', '[n-]', '[nH+]', '[nH]', \n",
    "            '[o+]', '[s+]', '[sH+]', '[se+]', '[se]', 'b', 'c', 'n', 'o', 'p', 's']\n",
    "\n",
    "        # 特殊トークン\n",
    "        self.special_tokens_list = ['<BOS>', '<EOS>', '<PAD>', '<UNK>']\n",
    "\n",
    "        # 全トークン\n",
    "        self.all_tokens = self.special_tokens_list + self.tokens\n",
    "        self.vocab = {token: idx for idx, token in enumerate(self.all_tokens)}\n",
    "        self.ids_to_tokens = {idx: token for token, idx in self.vocab.items()}\n",
    "\n",
    "        # 正規表現パターン\n",
    "        self.pattern = re.compile(\n",
    "            r'%\\d{2}|\\[[^\\]]+\\]|Cl|Br|Si|Se|As|B|C|N|O|F|P|S|I|[cnospb]|[0-9]|\\(|\\)|=|#|-|\\/|\\\\|\\.|<'\n",
    "        )\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self.pattern.findall(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab['<UNK>'])\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens.get(index, '<UNK>')\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        # 文字列に戻す（BOS, EOSなどは除く）\n",
    "        return ''.join([t for t in tokens if t not in self.special_tokens_list])\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids):\n",
    "        return [self.vocab['<BOS>']] + token_ids + [self.vocab['<EOS>']]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix=None):\n",
    "        import os\n",
    "        vocab_file = os.path.join(save_directory, (filename_prefix or \"\") + \"vocab.txt\")\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in self.all_tokens:\n",
    "                f.write(token + \"\\n\")\n",
    "        return (vocab_file,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
